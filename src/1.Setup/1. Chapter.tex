\chapter{Задача обучения с подкреплением}

Ясно, что обучение с учителем это не та модель <<обучения>>, которая свойственна интеллектуальным сущностям. Термин <<обучение>> здесь подменяет понятие интерполяции или, если уж на то пошло, построение алгоритмов неявным образом (<<смотрите, оно само обучилось, я сам ручками не прописывал, как кошечек от собачек отличать>>). К полноценному обучению, на которое способен не только человек, но и в принципе живые организмы, задачи классического машинного обучения имеют лишь косвенное отношение. Значит, нужна другая формализация понятия <<задачи, требующей интеллектуального решения>>, в которой обучение будет проводится не на опыте, заданном прецедентно (в виде обучающей выборки).

%\setlength{\columnsep}{0.7cm}
\begin{wrapfigure}{r}{0.4\linewidth}
%\vspace{-0.3cm}
\centering
\includegraphics[width=0.4\textwidth]{Images/agentenv.jpeg}
%\caption{Пример среды с двумя действиями.}
%\label{fig:env}
%\vspace{-2.6cm}
\end{wrapfigure}

Термин \emph{подкрепление} (\ENGLISH{reinforcement}) пришёл из \href{https://ru.wikipedia.org/wiki/Бихевиоризм}{поведенческой психологии} и обозначает награду или наказание за некоторый получившийся результат, зависящий не только от самих принятых решений, но и внешних, не обязательно подконтрольных, факторов. Под обучением здесь понимается поиск способов достичь желаемого результата \emph{методом проб и ошибок} (\ENGLISH{trial and error}), то есть попыток решить задачу и использование накопленного опыта для усовершенствования своей стратегии в будущем.

В данной главе будут введены основные определения и описана формальная постановка задачи. Под желаемым результатом мы далее будем понимать максимизацию некоторой скалярной величины, называемой \emph{наградой} (\ENGLISH{reward}). Интеллектуальную сущность (систему/робота/алгоритм), принимающую решения, будем называть \emph{агентом} (\ENGLISH{agent}). Агент взаимодействует с \emph{миром} (\ENGLISH{world}) или \emph{средой} (\ENGLISH{environment}), которая задаётся зависящим от времени \emph{состоянием} (\ENGLISH{state}). Агенту в каждый момент времени в общем случае доступно только некоторое \emph{наблюдение} (\ENGLISH{observation}) текущего состояния мира. Сам агент задаёт процедуру выбора \emph{действия} (\ENGLISH{action}) по доступным наблюдениям; эту процедуру далее будем называть \emph{стратегией} или \emph{политикой} (\ENGLISH{policy}). Процесс взаимодействия агента и среды задаётся \emph{динамикой среды} (\ENGLISH{world dynamics}), определяющей правила смены состояний среды во времени и генерации награды.

Буквы $s$, $a$, $r$ зарезервируем для состояний, действий и наград соответственно; буквой $t$ будем обозначать время в процессе взаимодействия. 

\import{1.Setup/}{1.1.MDP.tex}
\import{1.Setup/}{1.2.Algorithms.tex}